import torch
import torchvision
from torchvision import datasets

import matplotlib.pyplot as plt

dataset_to_method = {
  'MNIST': datasets.MNIST,
  'FashionMNIST': datasets.FashionMNIST,
  'KMNIST': datasets.KMNIST
}

def plot_grayscale_img(imgs, labels):
  fig = plt.figure()
  for i in range(15):
    plt.subplot(3, 5, i + 1)
    plt.imshow(imgs[i][0], cmap='gray')
    plt.title(f'Label: {labels[i]}')
    plt.xticks([])
    plt.yticks([])
  return fig

# @st.cache
def load_sample_data(dataset, transform):
  load_method = dataset_to_method[dataset]
  data = load_method(root='./data', train=False, download=True, transform=transform)
  loader = torch.utils.data.DataLoader(data, batch_size=15, shuffle=False)
  return loader

# TODO: to be removed later
def plot_fake_loss():
  epoch = range(200)
  fake_train_loss = [0.7449075711336698, 0.7531153992253317, 0.7424155183088418, 0.7287691692474374, 0.7150769790588102, 0.7018026040868693, 0.6890030583400651, 0.6766691480758946, 0.6647814968256692, 0.6533197584107301, 0.6422641400690042, 0.6315956551790283, 0.621296160621373, 0.611348353319768, 0.6017357556494257, 0.5924426954517156, 0.5834542823621269, 0.5747563814121166, 0.5663355846611877, 0.55817918150179, 0.550275128184243, 0.5426120170215392, 0.535179045654223, 0.527965986683866, 0.5209631579205792, 0.5141613934349326, 0.5075520155575629, 0.5011268079298042, 0.4948779896753701, 0.4887981907354455, 0.4828804283872586, 0.4771180849480165, 0.4715048866518696, 0.46603488367652185, 0.46070243128761207, 0.45550217206287935, 0.4504290191537214, 0.44547814053892276, 0.4406449442235843, 0.4359250643357074, 0.43131434807279323, 0.42680884345154063, 0.422404787814778, 0.4180985970511639, 0.413886855484848, 0.4097663063940824, 0.4057338431197102, 0.4017865007263528, 0.3979214481811856, 0.3941359810170589, 0.3904275144487191, 0.3867935769126978, 0.38323180400329715, 0.3797399327787666, 0.37631579641348617, 0.372957319173468, 0.3696625116940525, 0.36642946653998815, 0.36325635402949363, 0.36014141830507224, 0.3570829736350334, 0.35407940093078977, 0.3511291444659609, 0.3482307087843233, 0.34538265578448674, 0.34258360197001886, 0.3398322158545315, 0.3371272155119013, 0.3344673662625408, 0.3318514784871697, 0.3292784055601955, 0.3267470418952826, 0.3242563210962225, 0.3218052142066724, 0.31939272805275504, 0.31701790367291804, 0.3146798148298163, 0.31237756659933374, 0.31011029403217655, 0.30787716088376665, 0.30567735840846066, 0.30351010421432706, 0.30137464117503954, 0.29927023639556743, 0.29719618022864774, 0.2951517853391335, 0.2931363858135613, 0.29114933631240775, 0.28919001126267435, 0.28725780408858104, 0.2853521264783056, 0.2834724076847977, 0.28161809385884407, 0.2797886474126648, 0.2779835464124156, 0.2762022839980727, 0.27444436782927584, 0.2727093195557697, 0.2709966743111901, 0.2693059802289799, 0.2676367979793265, 0.26598870032605004, 0.26436127170243967, 0.26275410780510783, 0.2611668152049397, 0.2595990109743484, 0.25805032232997804, 0.2565203862901658, 0.2550088493464054, 0.253515367148174, 0.2520396042004702, 0.2505812335734698, 0.24913993662373338, 0.24771540272642578, 0.24630732901803204, 0.2449154201491075, 0.24353938804657554, 0.24217895168517026, 0.24083383686758505, 0.2395037760129656, 0.23818850795334617, 0.2368877777377079, 0.2356013364433027, 0.23432894099394014, 0.23307035398492673, 0.23182534351438003, 0.2305936830206347, 0.2293751511254964, 0.22816953148307817, 0.226976612634006, 0.22579618786474906, 0.22462805507188136, 0.22347201663105826, 0.2223278792705224, 0.22119545394895074, 0.2200745557374727, 0.2189650037056859, 0.21786662081151267, 0.2167792337947499, 0.21570267307415564, 0.21463677264794503, 0.21358136999755714, 0.21253630599456677, 0.21150142481061898, 0.21047657383027726, 0.20946160356666324, 0.2084563675797959, 0.20746072239751562, 0.20647452743890657, 0.20549764494011977, 0.2045299398825091, 0.20357127992299542, 0.20262153532657898, 0.20168057890091634, 0.2007482859328978, 0.19982453412714243, 0.19890920354634986, 0.1980021765534385, 0.19710333775540997, 0.19621257394887684, 0.1953297740671989, 0.1944548291291674, 0.1935876321891888, 0.19272807828891114, 0.19187606441025057, 0.19103148942976295, 0.19019425407432158, 0.1893642608780511, 0.18854141414048195, 0.18772561988587969, 0.18691678582371235, 0.18611482131021725, 0.18531963731103496, 0.18453114636486975, 0.183749262548148, 0.1829739014406382, 0.18220498009200883, 0.18144241698928446, 0.18068613202517841, 0.17993604646727446, 0.1791920829280249, 0.17845416533554664, 0.17772221890518247, 0.17699617011181662, 0.17627594666290403, 0.17556147747220577, 0.17485269263420136, 0.17414952339916237, 0.17345190214886283, 0.1727597623729081, 0.17207303864566578, 0.1713916666037827, 0.17071558292426117, 0.170044725303088, 0.1693790324343982, 0.1687184439901499, 0.16806290060030715, 0.16741234383350576, 0.16676671617819805, 0.16612596102424795]
  fake_val_loss = [0.7649605738092766, 0.7803297772973125, 0.7735997392193754, 0.7632785375909725, 0.7527107002959943, 0.7424378780248908, 0.7325343320540673, 0.7229979407627066, 0.7138145427424141, 0.7049685179749485, 0.6964445365736132, 0.6882278500859571, 0.6803043378840631, 0.6726605090552851, 0.6652834935640906, 0.658161028892517, 0.6512814437454726, 0.6446336395815105, 0.6382070705343978, 0.6319917222017303, 0.6259780897080618, 0.6201571553882603, 0.6145203663803318, 0.6090596123659915, 0.6037672036518148, 0.598635849743886, 0.5936586385341882, 0.5888290161872605, 0.5841407677904354, 0.5795879988098778, 0.575165117377123, 0.5708668174165363, 0.5666880626125732, 0.5626240712065639, 0.5586703016056027, 0.5548224387807036, 0.5510763814273746, 0.5474282298589204, 0.5438742746009361, 0.5404109856543756, 0.5370350023941118, 0.5337431240699648, 0.5305323008776163, 0.5273996255675428, 0.5243423255610762, 0.5213577555437926, 0.5184433905076891, 0.5155968192148677, 0.5128157380568061, 0.5100979452846305, 0.5074413355871503, 0.5048438949947123, 0.5023036960882401, 0.4998188934940431, 0.4973877196461708, 0.49500848079923254, 0.4926795532756795, 0.4903993799325683, 0.4881664668338054, 0.48597938011477615, 0.4838367430271269, 0.4817372331522723, 0.47967957977297543, 0.4776625613930197, 0.4756850033957041, 0.4737457758324664, 0.4718437913335567, 0.46997800313318905, 0.46814740320213516, 0.4663510204811628, 0.46458791920918074, 0.46285719734034964, 0.4611579850447977, 0.45948944328793107, 0.4578507624836651, 0.45624116121719716, 0.4546598850332394, 0.45310620528587575, 0.45157941804647966, 0.45007884306633383, 0.44860382279082295, 0.44715372142226334, 0.44572792402862293, 0.44432583569554546, 0.44294688071928123, 0.44159050183823856, 0.4402561595010502, 0.43894333116914347, 0.4376515106519536, 0.4363802074730176, 0.4351289462652948, 0.4338972661941628, 0.43268472040662526, 0.4314908755053635, 0.43031531104633075, 0.4291576190586827, 0.428017403585882, 0.4268942802469145, 0.4257878758165895, 0.42469782782395976, 0.4236237841679692, 0.42256540274946175, 0.4215223511187515, 0.4204943061379916, 0.41948095365761917, 0.41848198820620086, 0.4174971126930362, 0.41652603812290623, 0.41556848332239765, 0.41462417467725726, 0.4136928458802561, 0.4127742376890878, 0.4118680976938232, 0.4109741800934954, 0.4100922454813963, 0.40922206063868566, 0.40836339833594953, 0.4075160371423429, 0.4066797612419877, 0.4058543602573008, 0.4050396290789515, 0.4042353677021623, 0.40344138106907096, 0.4026574789169015, 0.4018834756316906, 0.4011191901073384, 0.400364445609749, 0.39961906964586086, 0.39888289383735637, 0.3981557537988467, 0.39743748902037196, 0.3967279427540104, 0.3960269619044581, 0.39533439692339334, 0.3946501017074875, 0.3939739334999181, 0.393305752795233, 0.39264542324744517, 0.3919928115812212, 0.3913477875060513, 0.39071022363328, 0.3900799953958839, 0.38945698097089737, 0.38884106120438333, 0.3882321195388468, 0.3876300419430044, 0.387034716843822, 0.38644603506073016, 0.38586388974193897, 0.3852881763027776, 0.3847187923659783, 0.3841556377038389, 0.3835986141821903, 0.3830476257061088, 0.38250257816730476, 0.3819633793931336, 0.3814299390971637, 0.38090216883125594, 0.3803799819390865, 0.37986329351108394, 0.3793520203407045, 0.37884608088202326, 0.3783453952085816, 0.3778498849734497, 0.37735947337046694, 0.37687408509661396, 0.37639364631548555, 0.3759180846218155, 0.3754473290070341, 0.37498130982580585, 0.37451995876353, 0.37406320880476374, 0.37361099420253646, 0.3731632504485352, 0.37271991424411843, 0.37228092347214725, 0.37184621716959243, 0.37141573550090384, 0.3709894197321113, 0.37056721220563665, 0.37014905631579126, 0.3697348964849408, 0.3693246781403143, 0.3689183476914372, 0.36851585250817015, 0.36811714089933206, 0.3677221620918942, 0.36733086621071964, 0.3669432042588442, 0.3665591280982652, 0.36617859043123885, 0.3658015447820599, 0.3654279454793158, 0.3650577476385957, 0.36469090714564517, 0.3643273806399502, 0.3639671254987389, 0.36361009982138914, 0.36325626241423103, 0.362905572775729]

  fig, ax = plt.subplots()
  ax.plot(epoch, fake_train_loss, label='Train Loss')
  ax.plot(epoch, fake_val_loss, label='Validation Loss')
  ax.set_xlabel('Epoch')
  ax.set_ylabel('NLL Loss')
  ax.set_title('Fake Loss Figure')
  ax.legend()
  return fig
